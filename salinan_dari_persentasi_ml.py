# -*- coding: utf-8 -*-
"""Salinan dari Persentasi-ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wMrKSpVY7Hs_pB7cy-6vVXlM1cnwp5J_
"""

from google.colab import drive
drive.mount('/content/drive')

import csv
file_path = "/content/drive/MyDrive/notes/data cust final1.csv"

with open(file_path, mode="r") as file:
  csv_reader = csv.reader(file)
  header = next(csv_reader)
  print(f"Header; {header}")

  for row in csv_reader:
    print(row)

"""# 1. Penjelasan Bisnis Pada Project Segmentasi Pelanggan

## Masalah Bisnis:

Memiliki bisnis Cafe tentu harus memahami karakteristik pelanggan, sangat penting untuk meningkatkan kepuasan dan loyalitas pelanggan. Dengan banyaknya variasi pelanggan berdasarkan preferensi, frekuensi kunjungan, dan pola pembelian, cafe perlu strategi yang lebih spesifik untuk setiap segmen pelanggan.

Oleh karena itu, kami membuat sebuah model Machine Learning untuk melakukan *Segmentasi Pelanggan* berdasarkan data observasi yang dikumpulkan saat mengunjungi beberapa cafe.

Data yang dikumpulkan mencakup variabel-variabel penting seperti:
- Frekuensi kunjungan pelanggan
- Rata-rata pengeluaran per kunjungan
- Waktu kunjungan (pagi/siang/malam)
- Jenis produk yang paling sering dibeli
- Feedback atau rating dari pelanggan

Tujuan dari project ini adalah:
- Mengelompokkan pelanggan ke dalam beberapa segmen berdasarkan kemiripan perilaku.
- Membantu cafe dalam membuat strategi pemasaran yang lebih tepat sasaran untuk tiap segmen.
- Meningkatkan pelayanan dan promosi yang sesuai dengan kebutuhan tiap kelompok pelanggan.

Dengan menggunakan metode Machine Learning seperti **Clustering (contohnya: K-Means)**, cafe dapat memahami pola pelanggan mereka secara lebih mendalam dan mengoptimalkan strategi bisnis berdasarkan data.

# 2. Collect The Data

Sumber Data:
Data dikumpulkan berdasarkan observasi langsung dengan melakukan kunjungan ke beberapa cafe. Kami mencatat berbagai fitur penting dari menu, harga, rating, suasana, serta pelayanan yang diberikan.

Metode:
- Visit langsung ke beberapa cafe.
- Melakukan pencatatan manual terhadap fitur-fitur yang relevan.
- Data kemudian dikompilasi ke dalam bentuk spreadsheet (CSV) untuk dianalisis lebih lanjut di Google Colab.

# 3. Exploratory Data Analysis

Pada tahap Exploratory Data Analysis, kami melakukan beberapa langkah untuk memahami data yang kami miliki.

Pertama, kami melihat struktur dataset dengan mengecek nama kolom, tipe data, serta beberapa sampel data.

Kedua, kami menghitung statistik deskriptif seperti rata-rata, minimum, maksimum, dan standar deviasi untuk masing-masing fitur, sehingga kami bisa memahami sebaran nilai di dalam data.

Ketiga, kami memeriksa apakah ada data yang kosong atau missing values. Hasilnya, dataset kami relatif bersih tanpa missing values yang berarti.

Selain itu, kami membuat visualisasi sederhana, seperti histogram untuk melihat distribusi variabel pengeluaran dan frekuensi kunjungan pelanggan, serta heatmap korelasi untuk memahami hubungan antar fitur.

Dari hasil EDA ini, kami menemukan bahwa pelanggan dengan frekuensi kunjungan lebih tinggi cenderung memiliki pengeluaran yang lebih besar, yang nantinya akan menjadi dasar dalam segmentasi pelanggan.
"""

# 1. Import library penting
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 2. Load dataset
df = pd.read_csv('/content/drive/MyDrive/notes/data cust final1.csv')

# 3. Lihat 5 baris pertama
print("Contoh Data Awal:")
display(df.head())

# 4. Info struktur data
print("Struktur Dataset:")
df.info()

# 5. Statistik deskriptif
print("Statistik Deskriptif:")
display(df.describe())

# 6. Cek missing value
print("Jumlah Missing Value di Setiap Kolom:")
print(df.isnull().sum())

# 7. Visualisasi EDA (Histogram, Rug Plot, Box Plot, Relational Plot)
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Histogram
plt.figure(figsize=(15,10))
for i, col in enumerate(numeric_columns):
    plt.subplot(3, 3, i+1)
    sns.histplot(df[col], kde=True, color='skyblue')
    plt.title(f'Histogram {col}')
plt.tight_layout()
plt.show()

# Rug Plot
plt.figure(figsize=(15,10))
for i, col in enumerate(numeric_columns):
    plt.subplot(3, 3, i+1)
    sns.rugplot(x=df[col], color='green')
    plt.title(f'Rug Plot {col}')
plt.tight_layout()
plt.show()

# Box Plot
plt.figure(figsize=(15,10))
for i, col in enumerate(numeric_columns):
    plt.subplot(3, 3, i+1)
    sns.boxplot(y=df[col], color='salmon')
    plt.title(f'Box Plot {col}')
plt.tight_layout()
plt.show()

# Relational Plot (Scatter plot antara dua fitur numerik pertama)
if len(numeric_columns) >= 2:
    plt.figure(figsize=(8,6))
    sns.scatterplot(data=df, x=numeric_columns[0], y=numeric_columns[1], hue=numeric_columns[0], palette='viridis')
    plt.title(f'Relational Plot antara {numeric_columns[0]} dan {numeric_columns[1]}')
    plt.show()
else:
    print("Tidak cukup fitur numerik untuk membuat relational plot.")

"""# 4. Data Cleaning"""

import pandas as pd
import numpy as np

# Load data dari file CSV
file_path = "/content/drive/MyDrive/notes/data cust final1.csv"  # Sesuaikan dengan path di Google Colab
df = pd.read_csv(file_path)

# Mmembuat dataframe contoh
df_filtered = df[df['Customer ID'].str.contains("901|902|903|904|905|906|907|908|909|910|911|912|913|914|915", na=False)]

# Menampilkan Data Awal
print("Data Awal:")
print(df.head())

import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('/content/drive/MyDrive/notes/data cust final1.csv')

# Drop duplicate
df = df.drop_duplicates()

# Handle missing values and data cleaning
df["Age"] = pd.to_numeric(df["Age"], errors='coerce')
median_age = df["Age"].median()
df["Age"] = df["Age"].apply(lambda x: median_age if pd.isna(x) or x < 0 or x > 100 else x)

df["Order Count"] = df["Order Count"].fillna(df["Order Count"].median())

df["Avg Order Value (Rp)"] = pd.to_numeric(df["Avg Order Value (Rp)"], errors='coerce')
median_order_value = df["Avg Order Value (Rp)"].median()
df["Avg Order Value (Rp)"] = df["Avg Order Value (Rp)"].fillna(median_order_value)

df["Favorite Food"] = df["Favorite Food"].fillna(df["Favorite Food"].mode()[0])

df["Favorite Drink"] = df["Favorite Drink"].replace("None", np.nan)
df["Favorite Drink"] = df["Favorite Drink"].fillna(df["Favorite Drink"].mode()[0])

df["Visit Frequency (per month)"] = df["Visit Frequency (per month)"].fillna(df["Visit Frequency (per month)"].median())

df["Holiday Visits"] = df["Holiday Visits"].fillna(df["Holiday Visits"].mode()[0])

df["Payment Method"] = df["Payment Method"].replace("INVALID", df["Payment Method"].mode()[0])

df["Referral Count"] = df["Referral Count"].apply(lambda x: df["Referral Count"].median() if x < 0 else x)

median_stay_duration = df["Stay Duration (minutes)"].median()
df["Stay Duration (minutes)"] = df["Stay Duration (minutes)"].apply(lambda x: median_stay_duration if x < 0 else x)

# Save cleaned dataset
output_path = '/content/drive/MyDrive/Colab Notebooks/data_cust_final1_cleaned.csv'
df.to_csv(output_path, index=False)

print(f'✅ Data cleaning complete! File saved at: {output_path}')

#menangani outlier
import numpy as np

# Identify numerical columns
numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Create an empty set to store outlier row indices
outlier_indices = set()

# Detect outliers using IQR for each numerical column
for col in numerical_cols:
    Q1 = df[col].quantile(0.25)  # 25th percentile
    Q3 = df[col].quantile(0.75)  # 75th percentile
    IQR = Q3 - Q1  # Interquartile range

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Find indices of rows where values are outliers
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    outlier_indices.update(outliers.index)  # Store the row indices

print(f"Total Outliers Detected: {len(outlier_indices)}")

# Function to handle null values
def handle_null_values(df):
    """Fills missing values:
       - Numerical: Uses median
       - Categorical: Uses mode"""
    num_cols = df.select_dtypes(include=['number']).columns
    cat_cols = df.select_dtypes(include=['object']).columns

    df[num_cols] = df[num_cols].apply(lambda x: x.fillna(x.median()))  # Fill numeric with median
    df[cat_cols] = df[cat_cols].apply(lambda x: x.fillna(x.mode()[0]))  # Fill categorical with mode

    print("\n✅ Null Values After Cleaning:")
    print(df.isnull().sum())
    return df

# Apply function
df = handle_null_values(df)

# Import library yang diperlukan
import pandas as pd

# Load dataset
file_path = "/content/drive/MyDrive/notes/data cust final1.csv"  # Ganti sesuai dengan lokasi file di Google Colab
df = pd.read_csv(file_path)

# Mengubah tipe data
df['Customer ID'] = df['Customer ID'].astype(str)
df['Gender'] = df['Gender'].astype('category')
df['Age'] = pd.to_numeric(df['Age'], errors='coerce')  # Mengubah ke numerik, mengatasi error
df['Avg Order Value (Rp)'] = df['Avg Order Value (Rp)'].replace('[^0-9.]', '', regex=True)
df['Avg Order Value (Rp)'] = pd.to_numeric(df['Avg Order Value (Rp)'], errors='coerce')
df['Favorite Food'] = df['Favorite Food'].astype('category')
df['Favorite Drink'] = df['Favorite Drink'].astype('category')
df['Payment Method'] = df['Payment Method'].astype('category')
df['Work Type'] = df['Work Type'].astype('category')
df['Preferred Ordering Method'] = df['Preferred Ordering Method'].astype('category')
df['Holiday Visits'] = df['Holiday Visits'].astype('category')
df['Time of Visit'] = df['Time of Visit'].astype('category')  # Mengonversi ke datetime

# Menampilkan tipe data setelah perubahan
print("Tipe Data Setelah Perubahan:")
print(df.dtypes)

# Menampilkan contoh data setelah perubahan
df.head()


save_path = "/content/drive/MyDrive/notes/data_cust_final1.csv"  # Sama dengan file asli
df.to_csv(save_path, index=False)
print(f"\n✅ Dataset berhasil diperbarui di: {save_path}")

#download setelah cleaning
from google.colab import files

# Define the cleaned file path in Google Drive
cleaned_file_path = "/content/drive/MyDrive/Colab Notebooks/data_cust_final1_cleaned.csv"

# Copy the file to Colab's local storage (for download)
download_file_path = "/content/data_cust_final1_cleaned.csv"
!cp "$cleaned_file_path" "$download_file_path"

# Download the file
files.download(download_file_path)

print(f"✅ File saved and ready for download: {download_file_path}")

"""## 5. Data Pre-Processing (choose at least 2)"""

# --- 5. Data Preprocessing
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# 1) Label Encoding kolom kategorikal
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# 2) Feature Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# 3) PCA untuk reduksi dimensi (optional tapi disarankan)
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

# Visualisasi explained variance
plt.plot(np.cumsum(PCA().fit(X_scaled).explained_variance_ratio_), marker='o')
plt.xlabel('Jumlah Komponen')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance oleh PCA')
plt.grid(True)
plt.show()

"""## 6. Pemilihan algoritma

Untuk tahap modeling, kami memilih menggunakan algoritma K-Means Clustering.

Alasan kami memilih K-Means adalah karena project ini bertujuan untuk melakukan segmentasi pelanggan atau mengelompokkan pelanggan berdasarkan kesamaan karakteristik.

K-Means merupakan salah satu algoritma unsupervised learning yang sangat populer untuk clustering, karena:

- Prosesnya sederhana dan cepat untuk dataset berukuran menengah.

- Dapat membentuk beberapa kelompok pelanggan berdasarkan jarak antar data.

- Cocok digunakan saat kita tidak memiliki label target, seperti dalam kasus segmentasi ini.

Dengan K-Means, kami berharap bisa membedakan pelanggan berdasarkan perilaku mereka, seperti frekuensi kunjungan, rata-rata pengeluaran, dan preferensi waktu kunjungan.

## 7. Teknik evaluasi yang digunakan

mengevaluasi hasil clustering.

Karena K-Means adalah metode unsupervised learning, kami tidak bisa mengevaluasi model dengan metrik seperti akurasi atau F1-score, seperti yang biasa digunakan dalam supervised learning.

Sebagai gantinya, kami menggunakan metode evaluasi khusus untuk clustering, yaitu Inertia dan Silhouette Score:

- Inertia (Within-Cluster Sum of Squares / WCSS) digunakan untuk mengukur seberapa dekat data dalam satu cluster. Semakin kecil inertia, semakin baik.

- Silhouette Score mengukur seberapa baik suatu titik data berada dalam cluster yang tepat, dibandingkan dengan cluster lainnya. Nilai mendekati 1 menunjukkan clustering yang baik.

Dari hasil evaluasi, kami menemukan bahwa jumlah cluster optimal berada di sekitar (Berapa) cluster.
"""

# --- 7. Teknik Evaluasi
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Cari jumlah cluster terbaik
range_k = range(2, 11)
silhouette_scores = []

for k in range_k:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X_pca)
    silhouette_scores.append(silhouette_score(X_pca, labels))

# Plot Silhouette Score
plt.plot(range_k, silhouette_scores, marker='o')
plt.xlabel('Jumlah Cluster (k)')
plt.ylabel('Silhouette Score')
plt.title('Evaluasi Cluster dengan Silhouette Score')
plt.grid(True)
plt.show()

# Menentukan jumlah cluster optimal
optimal_k = range_k[np.argmax(silhouette_scores)]
print(f"Jumlah cluster optimal: {optimal_k}")

# Build model final
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)
df['Cluster'] = kmeans_final.fit_predict(X_pca)

# Cek Silhouette Score Akhir
final_silhouette = silhouette_score(X_pca, df['Cluster'])
print(f"Final Silhouette Score: {final_silhouette:.2f}")

if final_silhouette >= 0.80:
    print("Score sudah di atas 80% ✅")
else:
    print("Score masih di bawah 80% ❌")

#PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Visualisasi hasil clustering setelah PCA
plt.figure(figsize=(10, 6))
sns.scatterplot(
    x=X_pca[:, 0],  # Komponen PCA 1
    y=X_pca[:, 1],  # Komponen PCA 2
    hue=df['Cluster'],  # Pewarnaan berdasarkan cluster
    palette='viridis',  # Pilih palet warna
    s=60  # Ukuran titik
)

plt.title('Visualisasi Clustering setelah PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend(title='Cluster')
plt.grid(True)
plt.show()

"""## 8. Cara melakukan tuning pada algoritma

Setelah mengevaluasi hasil clustering, kami menyesuaikan jumlah cluster yang optimal.

Untuk menentukan jumlah cluster terbaik, kami menggunakan dua teknik:

- Elbow Method, yaitu dengan melihat grafik Inertia (WCSS). Kita mencari titik di mana nilai inertia mulai melambat, menandakan jumlah cluster yang optimal.

- Silhouette Score, yang mengukur seberapa baik suatu data berada dalam cluster yang sesuai. Nilai yang lebih tinggi menunjukkan hasil clustering yang lebih baik.

Dari hasil tuning ini, kami menemukan bahwa jumlah cluster optimal adalah (Berapa) cluster.

Dengan jumlah cluster ini, segmentasi pelanggan menjadi lebih jelas, sehingga setiap kelompok pelanggan memiliki karakteristik yang unik dan dapat digunakan untuk strategi bisnis cafe."
"""